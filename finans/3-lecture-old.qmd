---
title: 3 - Optimal Portfolios and Matrices"
author: "Espen Sirnes"
date: "2024-9-18"
format:
  pdf:
    number-sections: true
    geometry: [left=4cm, right=4cm, top=4cm, bottom=4cm]
    fontsize: 12pt
    fontfamily: times
    monofont: "Courier New"
    documentclass: article
    toc: true
    fig-cap: true
    fig-pos: H
    titlepage: true
    titlepage-text-color: "000000"
    titlepage-rule-color: "000000"
    titlepage-rule-height: 2
---

This lecture explores the strategic behavior of an investor in the stock market, particularly under the assumption of risk aversion, as discussed in the previous note on utility theory. Risk lovers generally prefer the most risky assets, while risk-neutral investors opt for assets with the highest returns. In contrast, a risk-averse investor seeks to maximize returns without disproportionately increasing volatility, typically measured as variance.



# Matrices

To calculate optimal portfolios for any number of assets, a basic understanding of matrix algebra is essential. Matrix algebra simplifies the resolution of several equations simultaneously, a process that becomes increasingly complex with the addition of variables. Using matrix functions in software like Excel and various statistical packages allows us to solve systems of equations efficiently without manually computing each one.

Matrices not only streamline the computation but also simplify notation, making the formulation of equations for optimal portfolios more manageable.

A matrix is a structured array of numbers arranged in rows and columns, essentially a set of vectors. Here's an example of a vector:

```{python}
import numpy as np
np.random.randint(0,100,3)
```

Combining several vectors side-by-side forms a matrix:

```{python}
np.random.randint(0,100,(2,3))

```

This format is sometimes denoted as $\mathbf{X}_{N \times K}$ to indicate the number of rows ($N$) and columns ($K$).

# Algebra with Matrices

Matrix algebra operates under similar principles to ordinary algebra—allowing addition, subtraction, multiplication, and division (through inversion)—but it also requires adherence to specific rules.

## Matrix Multiplication

The core operation in matrix algebra is matrix multiplication, which combines elements from the rows of the first matrix with the columns of the second. For example, multiplying a $2 \times 3$ matrix by a $3 \times 2$ matrix yields:

```{python}
X = np.random.randint(0,5,(2,3))
Y = np.random.randint(0,5,(3,2))
result = np.dot(X, Y)
print(X)
print(Y)
print(result)
```

What happens is that we sum the product of the elements in each row of the first matrix and each column of the second. You can for example check that element [0,0] of the result is the sum of the product of the first row of the first matrix, and the first column of the second. An easy way to remember this is to think of the multiplication of $A \times B$ is to follow the lines of the letters:
![multiplication rule](img/multrule.png "Muliplication rule")

Due to the rules for matrix multiplication, it requires the number of columns in the first matrix to match the number of rows in the second.

The matrix multiplication is different from the normal multiplication in Python. Normal multiplicaiton can be done with the normal multiplication operator `*`. It will then multiply each element in X with the corresponding element of Y, and both matrices must be of the same size:

```{python}
X = np.random.randint(0,5,(2,3))
Y = np.random.randint(0,5,(2,3))
result = X*Y
print(X)
print(Y)
print(result)
```
The reason for using the former method, is that the former is required for solving sets of equations. 

## Adding and Subtracting Matrices

Adding or subtracting matrices is straightforward; simply add or subtract corresponding elements. In Python, the multiplication requires  numpy function, but if the matrices are numpy variables, subtraction and addition can be done with the normal operators. 

```{python}
import numpy as np

X = np.random.randint(0,100,(2,2))
Y = np.random.randint(0,100,(2,2))

# Addition of matrices
result_add = X + Y
print(X)
print(Y)
print(result_add)
```

## Dividing with a Matrix

While direct division isn't defined in matrix operations, we can achieve a similar result by multiplying by the inverse of a matrix. The inverse of a matrix $\mathbf{X}$, denoted $\mathbf{X}^{-1}$, satisfies:

$$
\mathbf{X} \times \mathbf{X}^{-1} = \mathbf{I} =
\begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}
$$

where $\mathbf{I}$ is the identity matrix. Multiplying any matrix by $\mathbf{I}$ results in the original matrix, akin to multiplying any number by 1.

In practice, while the concept is straightforward, the actual calculation of a matrix inverse can become complex for larger matrices and is typically handled by computers. We will not go through the method of obtaining the inverse in this course, we will in stead just utilize the numpy funciton for calculating the inverse. Specifically, we use `np.linalg.inv(X)`. We can check that it actually complies with the definition like this:

```{python}
X = np.random.randint(0,10,(3,3))
# Calculating inverse of X
X_inv = np.linalg.inv(X)

# Testing
np.round(np.dot(X_inv, X),1)
```


## Solving Equations with Matrix Algebra

The foundation we've established for matrix algebra now allows us to efficiently solve systems of equations. Consider solving the following pair of simultaneous equations:

$$
x_{11}a_{1} + x_{12}a_{2} = b_{1} \\
x_{21}a_{1} + x_{22}a_{2} = b_{2}
$$

Here, we know the values of $x$ and $b$ but need to find the values of $a$. These equations can be succinctly expressed using matrix notation:

$$
\mathbf{X \times a} = \mathbf{b}
$$

where $\mathbf{a}$ and $\mathbf{b}$ are column vectors. Let us define the right hand side vector `b` and the coeficient matrix `X` randomly in python as

```{python}
b =  np.random.randint(0,100,(2,1))
# Define matrix X
X =  np.random.randint(0,100,(2,2))
print(X)
print(b)
```

To solve for $\mathbf{a}$, we use the inverse of $\mathbf{X}$, provided it exists, and multiplies it with the left and right hand sides of the equation, just as we would divide with X on both sides to solve for a single equation:

$$
\mathbf{X}^{-1} \times \mathbf{X} \times \mathbf{a} = \mathbf{X}^{-1}\mathbf{b}
$$

Since we know that $\mathbf{X}^{-1}$ is the solution to $\mathbf{X}^{-1} \times \mathbf{X} = \mathbf{I}$, premultiplying with $\mathbf{X}^{-1}$ yields:

$$
\mathbf{a} = \mathbf{X}^{-1}\mathbf{b}
$$

Hence, we have found an easy way to solve any linear equation. We can test that it works in python. Let us first find `a` using this approach: 

```{python}
a = np.dot(np.linalg.inv(X), b)
a
```

If you get a "Singular matrix" error its because we are generating `X` with a few random integers, which sometimes creates unsolvable systems, so just generate `X` and `b` again.

Now we can test, if the solution for a actually works, by applying it on the original equation $\mathbf{X \times a} = \mathbf{b}$. This should yield the right hand side of th equation, `b`:

```{python}
np.dot(X, a)
```
 Compare this with the actual `b`:

```{python}
np.dot(X, a)
```
Thus, we have identified an effective method to solve any system of equations, provided that $\mathbf{X}$ is invertible. If $\mathbf{X}$ cannot be inverted, it indicates that two or more equations are essentially identical, leading to an "underdetermined" system. In such cases, some equations are redundant, and there are not enough independent equations to determine the values of all variables. Remember the fundamental rule: we need an equal number of equations and unknowns to uniquely solve for each variable.


## Transposing

Transposing a matrix involves swapping its rows and columns. For example, a $2 \times 3$ matrix:

$$
\mathbf{X}_{2 \times 3} =
\begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23}
\end{pmatrix}
$$

transposes to:

$$
\mathbf{X}_{2 \times 3}^{\prime} =
\begin{pmatrix}
x_{11} & x_{21} \\
x_{12} & x_{22} \\
x_{13} & x_{23}
\end{pmatrix}
$$

where $^{\prime}$ denotes the transposed matrix. For a column vector $\mathbf{a}$, transposing and then multiplying by itself, $\mathbf{a}^{\prime}\mathbf{a}$, calculates the sum of squares of its components.

```{python}
# Example of matrix transposition
X_2x3 = np.random.randint(0,100,(2,3))
X_transposed = X_2x3.T
X_transposed
```

Transposition is often used to conform to the requirements of matrix multiplication, where the number of columns in the first matrix must match the number of rows in the second. If this is not the case, one might transpose the first matrix to facilitate multiplication.

# Calculus and matrices

Deriving matrices follows similar principles to deriving polynomials. For instance:

$$
\frac{d\left( a^{2} \sigma^{2} \right)}{da} = 2a \sigma^{2}
$$

applies to scalar variables, and for a matrix $\mathbf{\Sigma}$ and a column vector $\mathbf{a}$, we have:

$$
\frac{d\left(\mathbf{a}^{\prime} \mathbf{\Sigma} \mathbf{a} \right)}{d \mathbf{a}^{\prime}} = 2 \mathbf{\Sigma}\mathbf{a}
$$

assuming $\mathbf{\Sigma}$ is symmetric. In practical terms, the derivative with respect to `a` here, given some values for `a`, is

```{python}
# Derivation with matrix and vector
a = np.random.randint(0,100,(2,1))
Sigma = np.random.randint(0,100,(2,2))

# Derivative of a' Σ a with respect to a
derivative = 2 * np.dot(Sigma, a)
derivative
```

We can rewrite the matrix formulation in scalar form, to check that the rule is correct. The scalar form of $\mathbf{a}^{\prime} \mathbf{\Sigma}\mathbf{a}$ is

$$
\mathbf{a}^{\prime} \mathbf{\Sigma} \mathbf{a} = \sum_{j=0}^{N} a_j \left( \sum_{i=0}^{N} a_i \sigma_{ij} \right)
$$

You can verify that 

$$
\frac{d(\mathbf{a}^{\prime} \mathbf{\Sigma} \mathbf{a}) }{d\mathbf{a}}= 2 [\sum_{i=0}^{N} a_i \sigma_{i0}, ..., \sum_{i=0}^{N} a_i \sigma_{iN}]
$$



# Optimal portfolios with more than one asset


We remember from above the previous chapter that with one asset, the optimal portfolio was 

$$
a=\frac{(mu -r)}{\lambda \sigma^2}
$$

From this we concluded that:

1. The more risk-averse the person is, the less they should invest.
2. The larger the expected return of the asset, the more should be invested.
3. The greater the risk associated with the asset, represented by $\sigma^2$, the less should be invested.

Now, let us consider the optimal investments if we have more than one asset.

## Optimal Portfolios with Any Number of Assets

Let us now assume that the investor in the previous section has a portfolio of $K$ assets, not just one. Their wealth next period, assuming the entire amount is borrowed, is then expressed in matrix notation as:

$$
W_1 = \mathbf{a}'\mathbf{x} - \mathbf{1}r
$$

where $\mathbf{a}$ represents the portfolio weights, $\mathbf{x}$ represents the returns, and $\mathbf{1}$ is a column vector of ones, such that $\mathbf{1}r$ is a column vector of the risk-free interest rate $r$. Recall from earlier that the investor aims to maximize the difference between expected return and variance:

$$
\max_{\mathbf{a}} Z = \mathbb{E}W_1 - \pi \frac{1}{2} \operatorname{var}(W_1)  
$$

$\mathbf{x}$ now is a column vector of many normally distributed variables with different variances and expectations. We denote the expected returns by $\mu_i$ for asset $i$, and the associated vector of these returns by $\mathbf{\mu}$. Given a portfolio $\mathbf{a}$, the expected return on the portfolio then becomes:

$$
\mathbb{E}W_1 = \mathbf{a}'(\mathbb{E}\mathbf{x}-\mathbf{1}r) = \mathbf{a}\mathbf{\mu} - \mathbf{1}r
$$


For the variance, the risk free return $r$ is not relevant, since means are subtracted anyway. We define the covariance matrix, all the combinations of variance and covariance between the stocks as

$$
\operatorname{var}W_1 = \mathbf{\Sigma} = 
\begin{bmatrix}
\sigma_0 & \sigma_1 & \cdots & \sigma_N \\
\sigma_1 & \sigma_2 & \cdots & \vdots \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_N & \cdots & \cdots & \sigma_{NN}
\end{bmatrix}
$$


Where $\sigma_{ij}$ is the covariance between $i$ and $j$, and $\sigma_i^2$ is the variance of asset $i$. This is the covariance matrix, denoted by the capital sigma, $\mathbf{\Sigma}$.

When a vector is normally distributed we write it as $\mathbf{x} \sim N(\mathbf{\mu}, \mathbf{\Sigma})$.

We have now derived expressions for $\mathbb{E}(W_1)$ and $\operatorname{var}(W_1)$ using matrix notation. Building on the concepts from the previous lecture, we can now formulate our portfolio optimization problem as:

$$
\max_{\mathbf{a}} Z = \mathbf{a}(\mathbf{\mu} - \mathbf{1}r) - \lambda \frac{1}{2} \mathbf{a}'\mathbf{\Sigma a}
$$

Taking the derivative with respect to $\mathbf{a}'$ yields the $K$ first order conditions:

$$
\frac{dZ}{d\mathbf{a}} = (\mathbf{\mu} - \mathbf{1}r) - \lambda \mathbf{\Sigma a} = 0
$$

Hence, in optimum:

$$
\mathbf{\Sigma a} = \frac{1}{\lambda}(\mathbf{\mu} - \mathbf{1}r)
$$

By premultiplying with the inverse of $\mathbf{\Sigma}$, we obtain the optimal portfolio:

$$
\mathbf{a} = \frac{1}{\lambda} \mathbf{\Sigma}^{-1}\mathbf{\mu} - \mathbf{1}r)
$$


Note that this formula looks very similar to the formula for an optimal portfolio with only one asset:

$$
a = \frac{(mu - r)}{\pi \sigma^2}
$$

In general, we may draw the same conclusions as in the case of one asset:

1. The more risk-averse the person is (large $\pi$), the less they should invest.
2. The larger the expected return the asset has, the more should be invested.
3. The more risk is associated with the asset, the less should be invested.

# Empirical example

Vi bruker scriptmuligheten i Titlon for å hente data

```{python}
#| tags: []


import pandas as pd
#Query script for Microsoft SQL Server (MSSQL) client
import pymssql
con = pymssql.connect(host='titlon.uit.no', 
                    user="esi000@uit.no", 
                    password  = "39oQ!Fjzpuh$OZ2FpewRp",
                    database='OSE')  
crsr=con.cursor()
crsr.execute("""
	SELECT  * FROM [OSE].[dbo].[equity] 
	WHERE year([Date]) >= 2016
	ORDER BY [Name],[Date]
""")
r=crsr.fetchall()
df=pd.DataFrame(list(r), columns=[i[0] for i in crsr.description])
pd.to_pickle(df,'output/stocks.df')
print(df)
df = None


#YOU NEED TO BE CONNECTED TO YOUR INSTITUTION VIA VPN, OR BE AT THE INSTITUTION, FOR THIS CODE TO WORK
```

## The portfolio front

When the numnber of stocks becomes large, the chances that a few of them ar essentially identical risk-wise increases. Therefore, in this example, the dimension is reduced by the function `get_independent_portfolios`, in a way that we will come back to. In a very simplified way, a matrix R is computed, that is multiplied with both the mean vector and the covariance matrix. 

When that is done, we can calulate the optimal portfolio (C) and the portfolio front. The portfolio front is the minum variance given any level of return.

```{python}
import functions
import pandas
import decomposition
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

MAX_AXIS = 0.8

df = pd.read_pickle('output/stocks.df')
# Defining risk free, cov matrix and mean vector
rf = df['NOWA_DayLnrate'].mean()*220


cov_matrix, means, df_month = functions.calc_moments(df)
df = None

# There are so many stocks, that many will be highly correlated 
# This creates a singular matrix (too few genuine equations)
# It is therefore neccessary to do a little trick to reduce the 
# Number of dimensions. Multiplying the matrix and means with 
# R, reduces the dimension
R = decomposition.get_independent_portfolios(cov_matrix, 0.0001)
cov_matrix = R.T @ cov_matrix @ R
means = R.T @ means

# Create a vector of ones with the same length as the number of columns in the covariance matrix
ones = np.ones(cov_matrix.shape[0])

# Defining the sum of the covariance elements:
A = np.dot(ones.T, np.dot(np.linalg.inv(cov_matrix), ones))

# Defining the sum of the optimal portfolio, in order to 
# normalize the portfolio size to one
B = np.dot(ones.T, np.dot(np.linalg.inv(cov_matrix), means-rf))[0]
print((A, B, C))
# Calculating the un-normalized optimal portfolio:
C = np.dot(means.T-rf, np.dot(np.linalg.inv(cov_matrix), means-rf))[0][0]

#Creating plot
fig, ax = plt.subplots(figsize=(10, 6))

# Setting the range of rp values and sigma values
rp_values = np.linspace(0, MAX_AXIS, 100)


# Calculate and plot sigma values for each rp
sigma_values = 1/A + ((rp_values - abs(B)/A)**2) / (C - B**2/A)
ax.plot(sigma_values**0.5, rp_values+rf, label='Efficient Frontier')

# Calculate the tangency point of the normalized optimal 
# portfolio and plotting it
tangency_rp = C/abs(B)
tangency_sigma =  1/A + ((tangency_rp - abs(B)/A)**2) / (C - B**2/A)

ax.plot(tangency_sigma**0.5, tangency_rp + rf, 'ro')

# Plotting the portfolio front
sigma_range = np.linspace(0, np.max(sigma_values**0.5), 100)
ax.plot(sigma_range, rf + 
	sigma_range*tangency_rp
	/tangency_sigma**0.5, color='r', linestyle='--', label='Tangency Point')

# Setting the axis ranges, lables, title and legend:
ax.set_xlim([0, 0.8])
ax.set_ylim([0, 0.5])
ax.set_xlabel('Sigma (Risk)')
ax.set_ylabel('Rp (Return)')
ax.set_title('Efficient Frontier')
ax.legend()
```

```{python}
np.max(sigma_values**0.5)
```